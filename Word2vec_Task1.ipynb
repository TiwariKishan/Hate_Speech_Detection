{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word2vec_Task1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOgXMQYQuUrpU6geyi2QNfi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TiwariKishan/Hate_Speech_Detection/blob/master/Word2vec_Task1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxYYrztlBODQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9ghoIwwBmZs",
        "colab_type": "code",
        "outputId": "dfa8a158-8010-4af9-c66d-8925c478f863",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGkSVaFhBprm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import division, print_function\n",
        "from gensim import models\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.layers import Dense, Dropout, Reshape, Flatten, concatenate, Input, Conv1D, GlobalMaxPooling1D, Embedding\n",
        "from keras.layers.recurrent import LSTM\n",
        "from keras.models import Sequential\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Model\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import collections\n",
        "import re\n",
        "import string"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kM1i0roI2UlP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pd.read_csv('/content/drive/My Drive/english_dataset.tsv', header = 0, sep='\\t')\n",
        "data=data.dropna()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ObV5bCI62wXr",
        "colab_type": "code",
        "outputId": "3f44bf20-78f9-4d48-8245-9051542f037a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        }
      },
      "source": [
        "data.columns = ['text_id','text', 'task_1','task_2','task_3']\n",
        "data=data.drop(['text_id'],axis=1)\n",
        "data=data.drop(['task_2'],axis=1)\n",
        "data=data.drop(['task_3'],axis=1)\n",
        "data.task_1.unique()\n",
        "data.head(500)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>task_1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>#DhoniKeepsTheGlove | WATCH: Sports Minister K...</td>\n",
              "      <td>NOT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>@politico No. We should remember very clearly ...</td>\n",
              "      <td>HOF</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>@cricketworldcup Guess who would be the winner...</td>\n",
              "      <td>NOT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Corbyn is too politically intellectual for #Bo...</td>\n",
              "      <td>NOT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>All the best to #TeamIndia for another swimmin...</td>\n",
              "      <td>NOT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>495</th>\n",
              "      <td>#fucktrump #impeachtrump ðŸ˜‚ðŸ˜‚ðŸ˜‚ðŸ˜‚ðŸ˜‚ðŸ˜‚ðŸ˜‚ @ Houston, Te...</td>\n",
              "      <td>NOT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>496</th>\n",
              "      <td>â€œEvery time I see Boris heâ€™s hanging from a wi...</td>\n",
              "      <td>NOT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>497</th>\n",
              "      <td>This one is understandable. #dickhead! https:/...</td>\n",
              "      <td>NOT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>498</th>\n",
              "      <td>#trendingnow #terrorist #murderer #dumptrump  ...</td>\n",
              "      <td>NOT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>499</th>\n",
              "      <td>A lip reader has reported what Trump was sayin...</td>\n",
              "      <td>HOF</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>500 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  text task_1\n",
              "0    #DhoniKeepsTheGlove | WATCH: Sports Minister K...    NOT\n",
              "1    @politico No. We should remember very clearly ...    HOF\n",
              "2    @cricketworldcup Guess who would be the winner...    NOT\n",
              "3    Corbyn is too politically intellectual for #Bo...    NOT\n",
              "4    All the best to #TeamIndia for another swimmin...    NOT\n",
              "..                                                 ...    ...\n",
              "495  #fucktrump #impeachtrump ðŸ˜‚ðŸ˜‚ðŸ˜‚ðŸ˜‚ðŸ˜‚ðŸ˜‚ðŸ˜‚ @ Houston, Te...    NOT\n",
              "496  â€œEvery time I see Boris heâ€™s hanging from a wi...    NOT\n",
              "497  This one is understandable. #dickhead! https:/...    NOT\n",
              "498  #trendingnow #terrorist #murderer #dumptrump  ...    NOT\n",
              "499  A lip reader has reported what Trump was sayin...    HOF\n",
              "\n",
              "[500 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUenKZ9h3RNr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data['task_1'] = data['task_1'].map({'NOT': 0, 'HOF': 1})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnwsHX-U3UbM",
        "colab_type": "code",
        "outputId": "6c8d4cbb-fefb-40c7-9512-f6492596878d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "data.task_1.unique()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOFQuE2N3dNj",
        "colab_type": "code",
        "outputId": "7d840633-090a-4b3f-d5c8-c3fe83957803",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "data.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5852, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8DQrwox3gkr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pos = []\n",
        "neg = []\n",
        "for l in data.task_1:\n",
        "    if l == 0:\n",
        "        pos.append(0)\n",
        "        neg.append(1)\n",
        "    elif l == 1:\n",
        "        pos.append(1)\n",
        "        neg.append(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFd1latX3k6z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data['Pos']= pos\n",
        "data['Neg']= neg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46EtoWGV3prj",
        "colab_type": "code",
        "outputId": "2846f86b-8da3-4e2c-ae7f-42e1724a4759",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>task_1</th>\n",
              "      <th>Pos</th>\n",
              "      <th>Neg</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>#DhoniKeepsTheGlove | WATCH: Sports Minister K...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>@politico No. We should remember very clearly ...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>@cricketworldcup Guess who would be the winner...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Corbyn is too politically intellectual for #Bo...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>All the best to #TeamIndia for another swimmin...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  task_1  Pos  Neg\n",
              "0  #DhoniKeepsTheGlove | WATCH: Sports Minister K...       0    0    1\n",
              "1  @politico No. We should remember very clearly ...       1    1    0\n",
              "2  @cricketworldcup Guess who would be the winner...       0    0    1\n",
              "3  Corbyn is too politically intellectual for #Bo...       0    0    1\n",
              "4  All the best to #TeamIndia for another swimmin...       0    0    1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVot0zRx3rmb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_punct(text):\n",
        "    text_nopunct = ''\n",
        "    text_nopunct = re.sub('['+string.punctuation+']', '', text)\n",
        "    return text_nopunct\n",
        "\n",
        "data['Text_Clean'] = data['text'].apply(lambda x: remove_punct(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMnwTYNq4QYK",
        "colab_type": "code",
        "outputId": "16b0c708-c214-448d-a3f8-d67fb4099493",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lr3rEKMc4hZL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk import word_tokenize, WordNetLemmatizer\n",
        "tokens = [word_tokenize(sen) for sen in data.Text_Clean]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGN76m7b5BYU",
        "colab_type": "code",
        "outputId": "8b351a76-1b54-4bf1-ef36-5780d7e8d3d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "def lower_token(tokens): \n",
        "    return [w.lower() for w in tokens]    \n",
        "    \n",
        "lower_tokens = [lower_token(token) for token in tokens]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-5467e797e8dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mlower_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlower_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'tokens' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6JvJcTL5Dzi",
        "colab_type": "code",
        "outputId": "51043965-19de-4330-8dae-5840948a80dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uGgieUpI5Joj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "stoplist = stopwords.words('english')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Okf_bkYS5fXU",
        "colab_type": "text"
      },
      "source": [
        "Function for removing all stopwords from the tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8V8z10M5NB7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_stop_words(tokens): \n",
        "    return [word for word in tokens if word not in stoplist]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEHTqeyY5bDL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filtered_words = [remove_stop_words(sen) for sen in lower_tokens]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bl3470H36bZ3",
        "colab_type": "text"
      },
      "source": [
        "join the words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIH_AmxY5eCN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result = [' '.join(sen) for sen in filtered_words]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsS8AWzv519s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data['Text_Final'] = result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_sMGDKN57Qj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data['tokens'] = filtered_words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4pbCEct6m6j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = data[['Text_Final', 'tokens', 'task_1', 'Pos', 'Neg']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZw_tWw260bb",
        "colab_type": "code",
        "outputId": "a2833a7f-dc99-4518-a943-1ab9f34b79d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "data[:47]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text_Final</th>\n",
              "      <th>tokens</th>\n",
              "      <th>task_1</th>\n",
              "      <th>Pos</th>\n",
              "      <th>Neg</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>dhonikeepstheglove watch sports minister kiren...</td>\n",
              "      <td>[dhonikeepstheglove, watch, sports, minister, ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>politico remember clearly individual1 admitted...</td>\n",
              "      <td>[politico, remember, clearly, individual1, adm...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>cricketworldcup guess would winner cwc19 team ...</td>\n",
              "      <td>[cricketworldcup, guess, would, winner, cwc19,...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>corbyn politically intellectual borisjohnsonsh...</td>\n",
              "      <td>[corbyn, politically, intellectual, borisjohns...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>best teamindia another swimming competition su...</td>\n",
              "      <td>[best, teamindia, another, swimming, competiti...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>kellymiller513 therealoj32 hope remembered wip...</td>\n",
              "      <td>[kellymiller513, therealoj32, hope, remembered...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>icc latest design wc2019 trophy cwc2019 cwc19 ...</td>\n",
              "      <td>[icc, latest, design, wc2019, trophy, cwc2019,...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>ados trendingnow blacklivesmatter justice fuck...</td>\n",
              "      <td>[ados, trendingnow, blacklivesmatter, justice,...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>thanks support wow 600k graffiti massive impac...</td>\n",
              "      <td>[thanks, support, wow, 600k, graffiti, massive...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>wearing balidaanbadge gloves msdhoni shown lov...</td>\n",
              "      <td>[wearing, balidaanbadge, gloves, msdhoni, show...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>mahendra singh dhonis love nation armed forces...</td>\n",
              "      <td>[mahendra, singh, dhonis, love, nation, armed,...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>â€™ know much take 45 compulsive liar trump30hou...</td>\n",
              "      <td>[â€™, know, much, take, 45, compulsive, liar, tr...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>shameoniccicc winner teamrain icc run teamrain...</td>\n",
              "      <td>[shameoniccicc, winner, teamrain, icc, run, te...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>narendramodi pmoindianamo narendramodi pmoindi...</td>\n",
              "      <td>[narendramodi, pmoindianamo, narendramodi, pmo...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>icc denies request bcci allow msd wear gloves ...</td>\n",
              "      <td>[icc, denies, request, bcci, allow, msd, wear,...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>good work icc keep going destroy whole fucking...</td>\n",
              "      <td>[good, work, icc, keep, going, destroy, whole,...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>wow youre full matthancock borisjohnsonshouldn...</td>\n",
              "      <td>[wow, youre, full, matthancock, borisjohnsonsh...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>indian spectators shd hv balidanbadge ground d...</td>\n",
              "      <td>[indian, spectators, shd, hv, balidanbadge, gr...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>icc bcci virat kholi tattoos ðŸ˜‚ðŸ˜‚ dhonikeepstheg...</td>\n",
              "      <td>[icc, bcci, virat, kholi, tattoos, ðŸ˜‚ðŸ˜‚, dhonike...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>important indiawithdhoni dhonikeepstheglove ht...</td>\n",
              "      <td>[important, indiawithdhoni, dhonikeepstheglove...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>kbmteam iowaspeedway hburtonracing rileyherbst...</td>\n",
              "      <td>[kbmteam, iowaspeedway, hburtonracing, rileyhe...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>harbor n bay mens boxer short pack 2 rs299 htt...</td>\n",
              "      <td>[harbor, n, bay, mens, boxer, short, pack, 2, ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>icc turns bccis request allow msdhoni continue...</td>\n",
              "      <td>[icc, turns, bccis, request, allow, msdhoni, c...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>shameonicc 1 icc dhonis gloves vs 2icc plannin...</td>\n",
              "      <td>[shameonicc, 1, icc, dhonis, gloves, vs, 2icc,...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>senior doctors rgkar greeted applause amp ovat...</td>\n",
              "      <td>[senior, doctors, rgkar, greeted, applause, am...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>empty podiums make much noise toryleadershipde...</td>\n",
              "      <td>[empty, podiums, make, much, noise, toryleader...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>shameonicc iccworldcup2019 world cup reality h...</td>\n",
              "      <td>[shameonicc, iccworldcup2019, world, cup, real...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>disgusting â€™ change culture come vetting syste...</td>\n",
              "      <td>[disgusting, â€™, change, culture, come, vetting...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>goqii vital colour display blood pressure moni...</td>\n",
              "      <td>[goqii, vital, colour, display, blood, pressur...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>500 doctors already resigned bengal nrs 100 ss...</td>\n",
              "      <td>[500, doctors, already, resigned, bengal, nrs,...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>hi everyone im kolkata thought id let know cra...</td>\n",
              "      <td>[hi, everyone, im, kolkata, thought, id, let, ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>bcci icc englandcricket worst ever world cup a...</td>\n",
              "      <td>[bcci, icc, englandcricket, worst, ever, world...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>realdonaldtrump fuck go fuck piece shit someon...</td>\n",
              "      <td>[realdonaldtrump, fuck, go, fuck, piece, shit,...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>donald trump lies trumpisatraitor happy johnmc...</td>\n",
              "      <td>[donald, trump, lies, trumpisatraitor, happy, ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>therealoj32 retired nfl players â€™ suffer cte b...</td>\n",
              "      <td>[therealoj32, retired, nfl, players, â€™, suffer...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>telegraphÂ´s use word Â´crownÂ´ regard boris john...</td>\n",
              "      <td>[telegraphÂ´s, use, word, Â´crownÂ´, regard, bori...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>doctorsfightback doctor wishes stop emergency ...</td>\n",
              "      <td>[doctorsfightback, doctor, wishes, stop, emerg...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>timesofindia worries timesofindia one winner r...</td>\n",
              "      <td>[timesofindia, worries, timesofindia, one, win...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>imro45 score 100 today predict httpstcomiljuka...</td>\n",
              "      <td>[imro45, score, 100, today, predict, httpstcom...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>dont support country concentration camps again...</td>\n",
              "      <td>[dont, support, country, concentration, camps,...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>dhonikeepsthegloveicc zero without dhoni like ...</td>\n",
              "      <td>[dhonikeepsthegloveicc, zero, without, dhoni, ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>ashwinnagar mamataofficial interest puplic spe...</td>\n",
              "      <td>[ashwinnagar, mamataofficial, interest, puplic...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>remaining matches get washed nz 12 ind 11 eng ...</td>\n",
              "      <td>[remaining, matches, get, washed, nz, 12, ind,...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>werdnat crazy manbaby probably paint gold gawd...</td>\n",
              "      <td>[werdnat, crazy, manbaby, probably, paint, gol...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>everything fucktrump httpstcoe2c48u3pss</td>\n",
              "      <td>[everything, fucktrump, httpstcoe2c48u3pss]</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>amishdevgan ima amp associations doctors hound...</td>\n",
              "      <td>[amishdevgan, ima, amp, associations, doctors,...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>hey trump lovely ladies got something tell ya ...</td>\n",
              "      <td>[hey, trump, lovely, ladies, got, something, t...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           Text_Final  ... Neg\n",
              "0   dhonikeepstheglove watch sports minister kiren...  ...   1\n",
              "1   politico remember clearly individual1 admitted...  ...   0\n",
              "2   cricketworldcup guess would winner cwc19 team ...  ...   1\n",
              "3   corbyn politically intellectual borisjohnsonsh...  ...   1\n",
              "4   best teamindia another swimming competition su...  ...   1\n",
              "5   kellymiller513 therealoj32 hope remembered wip...  ...   1\n",
              "6   icc latest design wc2019 trophy cwc2019 cwc19 ...  ...   1\n",
              "7   ados trendingnow blacklivesmatter justice fuck...  ...   0\n",
              "8   thanks support wow 600k graffiti massive impac...  ...   1\n",
              "9   wearing balidaanbadge gloves msdhoni shown lov...  ...   1\n",
              "10  mahendra singh dhonis love nation armed forces...  ...   1\n",
              "11  â€™ know much take 45 compulsive liar trump30hou...  ...   0\n",
              "12  shameoniccicc winner teamrain icc run teamrain...  ...   1\n",
              "13  narendramodi pmoindianamo narendramodi pmoindi...  ...   1\n",
              "14  icc denies request bcci allow msd wear gloves ...  ...   1\n",
              "15  good work icc keep going destroy whole fucking...  ...   0\n",
              "16  wow youre full matthancock borisjohnsonshouldn...  ...   1\n",
              "17  indian spectators shd hv balidanbadge ground d...  ...   1\n",
              "18  icc bcci virat kholi tattoos ðŸ˜‚ðŸ˜‚ dhonikeepstheg...  ...   1\n",
              "19  important indiawithdhoni dhonikeepstheglove ht...  ...   1\n",
              "20  kbmteam iowaspeedway hburtonracing rileyherbst...  ...   1\n",
              "21  harbor n bay mens boxer short pack 2 rs299 htt...  ...   1\n",
              "22  icc turns bccis request allow msdhoni continue...  ...   1\n",
              "23  shameonicc 1 icc dhonis gloves vs 2icc plannin...  ...   0\n",
              "24  senior doctors rgkar greeted applause amp ovat...  ...   1\n",
              "25  empty podiums make much noise toryleadershipde...  ...   1\n",
              "26  shameonicc iccworldcup2019 world cup reality h...  ...   1\n",
              "27  disgusting â€™ change culture come vetting syste...  ...   0\n",
              "28  goqii vital colour display blood pressure moni...  ...   1\n",
              "29  500 doctors already resigned bengal nrs 100 ss...  ...   1\n",
              "30  hi everyone im kolkata thought id let know cra...  ...   1\n",
              "31  bcci icc englandcricket worst ever world cup a...  ...   1\n",
              "32  realdonaldtrump fuck go fuck piece shit someon...  ...   0\n",
              "33  donald trump lies trumpisatraitor happy johnmc...  ...   1\n",
              "34  therealoj32 retired nfl players â€™ suffer cte b...  ...   0\n",
              "35  telegraphÂ´s use word Â´crownÂ´ regard boris john...  ...   1\n",
              "36  doctorsfightback doctor wishes stop emergency ...  ...   1\n",
              "37  timesofindia worries timesofindia one winner r...  ...   1\n",
              "38  imro45 score 100 today predict httpstcomiljuka...  ...   1\n",
              "39  dont support country concentration camps again...  ...   1\n",
              "40  dhonikeepsthegloveicc zero without dhoni like ...  ...   1\n",
              "41  ashwinnagar mamataofficial interest puplic spe...  ...   1\n",
              "42  remaining matches get washed nz 12 ind 11 eng ...  ...   1\n",
              "43  werdnat crazy manbaby probably paint gold gawd...  ...   0\n",
              "44            everything fucktrump httpstcoe2c48u3pss  ...   0\n",
              "45  amishdevgan ima amp associations doctors hound...  ...   1\n",
              "46  hey trump lovely ladies got something tell ya ...  ...   0\n",
              "\n",
              "[47 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-d158_d47DFt",
        "colab_type": "text"
      },
      "source": [
        "Split the data into trainning and testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ulwc4-763Ct",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_train, data_test = train_test_split(data, test_size=0.10, random_state=54)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OpOtJno7Cfb",
        "colab_type": "code",
        "outputId": "246c50d4-752d-4632-88e7-3ec7433bdf43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "all_training_words = [word for tokens in data_train[\"tokens\"] for word in tokens]\n",
        "training_sentence_lengths = [len(tokens) for tokens in data_train[\"tokens\"]]\n",
        "TRAINING_VOCAB = sorted(list(set(all_training_words)))\n",
        "print(\"%s words total, with a vocabulary size of %s\" % (len(all_training_words), len(TRAINING_VOCAB)))\n",
        "print(\"Max sentence length is %s\" % max(training_sentence_lengths))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "81959 words total, with a vocabulary size of 19925\n",
            "Max sentence length is 77\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkLV7l2xF-n6",
        "colab_type": "code",
        "outputId": "9eac37a0-93c0-4dd0-d656-03c8941e97bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "all_test_words = [word for tokens in data_test[\"tokens\"] for word in tokens]\n",
        "test_sentence_lengths = [len(tokens) for tokens in data_test[\"tokens\"]]\n",
        "TEST_VOCAB = sorted(list(set(all_test_words)))\n",
        "print(\"%s words total, with a vocabulary size of %s\" % (len(all_test_words), len(TEST_VOCAB)))\n",
        "print(\"Max sentence length is %s\" % max(test_sentence_lengths))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9041 words total, with a vocabulary size of 4277\n",
            "Max sentence length is 68\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OK-mx8HRGFG9",
        "colab_type": "code",
        "outputId": "9998c4fd-6376-4278-ef37-7f169a8b2fb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "word2vec_path = '/content/drive/My Drive/General/GoogleNews-vectors-negative300.bin.gz'\n",
        "word2vec = models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1T2nTUK0GraA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_average_word2vec(tokens_list, vector, generate_missing=False, k=300):\n",
        "    if len(tokens_list)<1:\n",
        "        return np.zeros(k)\n",
        "    if generate_missing:\n",
        "        vectorized = [vector[word] if word in vector else np.random.rand(k) for word in tokens_list]\n",
        "    else:\n",
        "        vectorized = [vector[word] if word in vector else np.zeros(k) for word in tokens_list]\n",
        "    length = len(vectorized)\n",
        "    summed = np.sum(vectorized, axis=0)\n",
        "    averaged = np.divide(summed, length)\n",
        "    return averaged\n",
        "\n",
        "def get_word2vec_embeddings(vectors, clean_comments, generate_missing=False):\n",
        "    embeddings = clean_comments['tokens'].apply(lambda x: get_average_word2vec(x, vectors, \n",
        "                                                                                generate_missing=generate_missing))\n",
        "    return list(embeddings)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ar1T1ZZPIFnw",
        "colab_type": "text"
      },
      "source": [
        "Embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FV3IRhq2H1HI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_embeddings = get_word2vec_embeddings(word2vec, data_train, generate_missing=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZvKJRGIH_Au",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_SEQUENCE_LENGTH = 50\n",
        "EMBEDDING_DIM = 300"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qaiexG_wIWLl",
        "colab_type": "code",
        "outputId": "747ac78f-0b8a-447b-b4a3-9e420b4d768d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tokenizer = Tokenizer(num_words=len(TRAINING_VOCAB), lower=True, char_level=False)\n",
        "tokenizer.fit_on_texts(data_train[\"Text_Final\"].tolist())\n",
        "training_sequences = tokenizer.texts_to_sequences(data_train[\"Text_Final\"].tolist())\n",
        "\n",
        "train_word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(train_word_index))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 19918 unique tokens.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6IPQCTpIb7l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_cnn_data = pad_sequences(training_sequences, maxlen=MAX_SEQUENCE_LENGTH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSt06jBYIemW",
        "colab_type": "code",
        "outputId": "85578227-f04e-45b6-fd4e-b87ca8f0d67a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_embedding_weights = np.zeros((len(train_word_index)+1, EMBEDDING_DIM))\n",
        "for word,index in train_word_index.items():\n",
        "    train_embedding_weights[index,:] = word2vec[word] if word in word2vec else np.random.rand(EMBEDDING_DIM)\n",
        "print(train_embedding_weights.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(19919, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPY6hSnyIjyn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_sequences = tokenizer.texts_to_sequences(data_test[\"Text_Final\"].tolist())\n",
        "test_cnn_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6Xr4gibInEN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "   def ConvNet(embeddings, max_sequence_length, num_words, embedding_dim, labels_index): \n",
        "    embedding_layer = Embedding(num_words,\n",
        "                            embedding_dim,\n",
        "                            weights=[embeddings],\n",
        "                            input_length=max_sequence_length,\n",
        "                            trainable=False)\n",
        "    \n",
        "    sequence_input = Input(shape=(max_sequence_length,), dtype='int32')\n",
        "    embedded_sequences = embedding_layer(sequence_input)\n",
        "\n",
        "    convs = []\n",
        "    filter_sizes = [2,3,4,5,6]\n",
        "\n",
        "    for filter_size in filter_sizes:\n",
        "        l_conv = Conv1D(filters=200, kernel_size=filter_size, activation='relu')(embedded_sequences)\n",
        "        l_pool = GlobalMaxPooling1D()(l_conv)\n",
        "        convs.append(l_pool)\n",
        "\n",
        "\n",
        "    l_merge = concatenate(convs, axis=1)\n",
        "\n",
        "    x = Dropout(0.1)(l_merge)  \n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "    preds = Dense(labels_index, activation='sigmoid')(x)\n",
        "\n",
        "    model = Model(sequence_input, preds)\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                  optimizer='adam',\n",
        "                  metrics=['acc'])\n",
        "    model.summary()\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJ2bane1IqdH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "label_names = ['Pos', 'Neg']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrK06YUcIsv-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train = data_train[label_names].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wy6QHQR8IvQm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = train_cnn_data\n",
        "y_tr = y_train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9xDaWKBIxqu",
        "colab_type": "code",
        "outputId": "06bd2243-eada-4de7-f539-4007d942a2df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 826
        }
      },
      "source": [
        "model = ConvNet(train_embedding_weights, MAX_SEQUENCE_LENGTH, len(train_word_index)+1, EMBEDDING_DIM, \n",
        "                len(list(label_names)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            (None, 50)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_2 (Embedding)         (None, 50, 300)      5975700     input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_6 (Conv1D)               (None, 49, 200)      120200      embedding_2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_7 (Conv1D)               (None, 48, 200)      180200      embedding_2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_8 (Conv1D)               (None, 47, 200)      240200      embedding_2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_9 (Conv1D)               (None, 46, 200)      300200      embedding_2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_10 (Conv1D)              (None, 45, 200)      360200      embedding_2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_6 (GlobalM (None, 200)          0           conv1d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_7 (GlobalM (None, 200)          0           conv1d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_8 (GlobalM (None, 200)          0           conv1d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_9 (GlobalM (None, 200)          0           conv1d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_10 (Global (None, 200)          0           conv1d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 1000)         0           global_max_pooling1d_6[0][0]     \n",
            "                                                                 global_max_pooling1d_7[0][0]     \n",
            "                                                                 global_max_pooling1d_8[0][0]     \n",
            "                                                                 global_max_pooling1d_9[0][0]     \n",
            "                                                                 global_max_pooling1d_10[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 1000)         0           concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 128)          128128      dropout_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, 128)          0           dense_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 2)            258         dropout_4[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 7,305,086\n",
            "Trainable params: 1,329,386\n",
            "Non-trainable params: 5,975,700\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otzZs9ULIz19",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_epochs = 6\n",
        "batch_size = 23"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KtAM0fwZI3O2",
        "colab_type": "code",
        "outputId": "92b53788-aa71-434b-f4f1-067c08c3e273",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        }
      },
      "source": [
        "hist = model.fit(x_train, y_tr, epochs=num_epochs, validation_split=0.1, shuffle=True, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 4739 samples, validate on 527 samples\n",
            "Epoch 1/6\n",
            "2714/4739 [================>.............] - ETA: 13s - loss: 0.6958 - acc: 0.5986"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-94-156de2f3b479>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1176\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m     def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    202\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2977\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2979\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2980\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2981\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2935\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2937\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2938\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3zTmS9NI-up",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = model.predict(test_cnn_data, batch_size=1024, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KH2axAr2JFYp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels = [1, 0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srM1xgAcJW3e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prediction_labels=[]\n",
        "for p in predictions:\n",
        "    prediction_labels.append(labels[np.argmax(p)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3P16panJZhn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sum(data_test.task_1==prediction_labels)/len(prediction_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_m-bw1QJc6h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_test.task_1.value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCWboKxJJpKB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import classification_report,confusion_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbyfOKLtJsqO",
        "colab_type": "code",
        "outputId": "62f8259e-2257-4a00-fdad-ebf2ce43a67f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        }
      },
      "source": [
        "print(classification_report(data_test.task_1,prediction_labels))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.69      0.70      0.69       364\n",
            "           1       0.49      0.48      0.48       222\n",
            "\n",
            "    accuracy                           0.61       586\n",
            "   macro avg       0.59      0.59      0.59       586\n",
            "weighted avg       0.61      0.61      0.61       586\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZSHa2fKJuxh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7NQB3vRKIVi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}