{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Character_level_embedding_ENG_Task1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "mount_file_id": "11rUjAQ3TBLVHlD6tc0UYZtKkyN6LQFqF",
      "authorship_tag": "ABX9TyMgt67xYaG66FMJKZk3oHcJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TiwariKishan/Hate_Speech_Detection/blob/master/Character_level_embedding_ENG_Task1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WvPXDPdFp-d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sklearn\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "# Preprocessing\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk import punkt\n",
        "\n",
        "# Modeling\n",
        "import keras\n",
        "from keras.models import Model, load_model\n",
        "from keras.layers import Dense, Input, Dropout, MaxPooling1D, Conv1D, GlobalMaxPool1D, Bidirectional\n",
        "from keras.layers import LSTM, Lambda, Bidirectional, concatenate, BatchNormalization, Embedding\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.optimizers import Adam\n",
        "import tensorflow as tf\n",
        "import keras.backend as K\n",
        "\n",
        "import IPython\n",
        "from IPython.display import SVG\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZc6_mwaGhdE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_raw_data():\n",
        "    df = pd.read_csv(\n",
        "        \"/content/drive/My Drive/english_dataset.tsv\", \n",
        "        sep='\\t', \n",
        "        names=['text_id'\t,'text',\t'task_1',\t'task_2',\t'task_3']\n",
        "    )\n",
        "    \n",
        "    return df[['text',\t'task_1']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WxS87dOwHtxe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "aab51e5b-28a4-45dd-9c4a-e6748b9094f7"
      },
      "source": [
        "df = get_raw_data()\n",
        "df.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>task_1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>text</td>\n",
              "      <td>task_1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>#DhoniKeepsTheGlove | WATCH: Sports Minister K...</td>\n",
              "      <td>NOT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>@politico No. We should remember very clearly ...</td>\n",
              "      <td>HOF</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>@cricketworldcup Guess who would be the winner...</td>\n",
              "      <td>NOT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Corbyn is too politically intellectual for #Bo...</td>\n",
              "      <td>NOT</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  task_1\n",
              "0                                               text  task_1\n",
              "1  #DhoniKeepsTheGlove | WATCH: Sports Minister K...     NOT\n",
              "2  @politico No. We should remember very clearly ...     HOF\n",
              "3  @cricketworldcup Guess who would be the winner...     NOT\n",
              "4  Corbyn is too politically intellectual for #Bo...     NOT"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZAPmZh6H-th",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "28b4f4c7-3d7e-4aea-fe9c-8e8a8ed298ba"
      },
      "source": [
        "print('Number of news: %d' % (len(df)))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of news: 5853\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ni0N9PsEIR6Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df, test_df = train_test_split(df, test_size=0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKKhCVW_IUNR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CharCNN:\n",
        "  \n",
        "    \n",
        "    CHAR_DICT = 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789 .!?:,\\'%-\\(\\)/$|&;[]\"'\n",
        "    \n",
        "    def __init__(self, max_len_of_sentence, max_num_of_setnence, verbose=10):\n",
        "        self.max_len_of_sentence = max_len_of_sentence\n",
        "        self.max_num_of_setnence = max_num_of_setnence\n",
        "        self.verbose = verbose\n",
        "        \n",
        "        self.num_of_char = 0\n",
        "        self.num_of_label = 0\n",
        "        self.unknown_label = ''\n",
        "        \n",
        "    def build_char_dictionary(self, char_dict=None, unknown_label='UNK'):\n",
        "        \"\"\"\n",
        "            Define possbile char set. Using \"UNK\" if character does not exist in this set\n",
        "        \"\"\" \n",
        "        \n",
        "        if char_dict is None:\n",
        "            char_dict = self.CHAR_DICT\n",
        "            \n",
        "        self.unknown_label = unknown_label\n",
        "\n",
        "        chars = []\n",
        "\n",
        "        for c in char_dict:\n",
        "            chars.append(c)\n",
        "\n",
        "        chars = list(set(chars))\n",
        "        \n",
        "        chars.insert(0, unknown_label)\n",
        "\n",
        "        self.num_of_char = len(chars)\n",
        "        self.char_indices = dict((c, i) for i, c in enumerate(chars))\n",
        "        self.indices_char = dict((i, c) for i, c in enumerate(chars))\n",
        "        \n",
        "        if self.verbose > 5:\n",
        "            print('Totoal number of chars:', self.num_of_char)\n",
        "\n",
        "            print('First 3 char_indices sample:', {k: self.char_indices[k] for k in list(self.char_indices)[:3]})\n",
        "            print('First 3 indices_char sample:', {k: self.indices_char[k] for k in list(self.indices_char)[:3]})\n",
        "            \n",
        "\n",
        "        return self.char_indices, self.indices_char, self.num_of_char\n",
        "    \n",
        "    def convert_labels(self, labels):\n",
        "        \"\"\"\n",
        "            Convert label to numeric\n",
        "        \"\"\"\n",
        "        self.label2indexes = dict((l, i) for i, l in enumerate(labels))\n",
        "        self.index2labels = dict((i, l) for i, l in enumerate(labels))\n",
        "\n",
        "        if self.verbose > 5:\n",
        "            print('Label to Index: ', self.label2indexes)\n",
        "            print('Index to Label: ', self.index2labels)\n",
        "            \n",
        "        self.num_of_label = len(self.label2indexes)\n",
        "\n",
        "        return self.label2indexes, self.index2labels\n",
        "    \n",
        "    def _transform_raw_data(self, df, x_col, y_col, label2indexes=None, sample_size=None):\n",
        "        \"\"\"\n",
        "            ##### Transform raw data to list\n",
        "        \"\"\"\n",
        "        \n",
        "        x = []\n",
        "        y = []\n",
        "\n",
        "        actual_max_sentence = 0\n",
        "        \n",
        "        if sample_size is None:\n",
        "            sample_size = len(df)\n",
        "\n",
        "        for i, row in df.head(sample_size).iterrows():\n",
        "            x_data = row[x_col]\n",
        "            y_data = row[y_col]\n",
        "\n",
        "            sentences = sent_tokenize(x_data)\n",
        "            x.append(sentences)\n",
        "\n",
        "            if len(sentences) > actual_max_sentence:\n",
        "                actual_max_sentence = len(sentences)\n",
        "\n",
        "            y.append(label2indexes[y_data])\n",
        "\n",
        "        if self.verbose > 5:\n",
        "            print('Number of news: %d' % (len(x)))\n",
        "            print('Actual max sentence: %d' % actual_max_sentence)\n",
        "\n",
        "        return x, y\n",
        "    \n",
        "    def _transform_training_data(self, x_raw, y_raw, max_len_of_sentence=None, max_num_of_setnence=None):\n",
        "        \"\"\"\n",
        "            ##### Transform preorcessed data to numpy\n",
        "        \"\"\"\n",
        "        unknown_value = self.char_indices[self.unknown_label]\n",
        "        \n",
        "        x = np.ones((len(x_raw), max_num_of_setnence, max_len_of_sentence), dtype=np.int64) * unknown_value\n",
        "        y = np.array(y_raw)\n",
        "        \n",
        "        if max_len_of_sentence is None:\n",
        "            max_len_of_sentence = self.max_len_of_sentence\n",
        "        if max_num_of_setnence is None:\n",
        "            max_num_of_setnence = self.max_num_of_setnence\n",
        "\n",
        "        for i, doc in enumerate(x_raw):\n",
        "            for j, sentence in enumerate(doc):\n",
        "                if j < max_num_of_setnence:\n",
        "                    for t, char in enumerate(sentence[-max_len_of_sentence:]):\n",
        "                        if char not in self.char_indices:\n",
        "                            x[i, j, (max_len_of_sentence-1-t)] = self.char_indices['UNK']\n",
        "                        else:\n",
        "                            x[i, j, (max_len_of_sentence-1-t)] = self.char_indices[char]\n",
        "\n",
        "        return x, y\n",
        "\n",
        "    def _build_character_block(self, block, dropout=0.3, filters=[64, 100], kernel_size=[3, 3], \n",
        "                         pool_size=[2, 2], padding='valid', activation='relu', \n",
        "                         kernel_initializer='glorot_normal'):\n",
        "        \n",
        "        for i in range(len(filters)):\n",
        "            block = Conv1D(\n",
        "                filters=filters[i], kernel_size=kernel_size[i],\n",
        "                padding=padding, activation=activation, kernel_initializer=kernel_initializer)(block)\n",
        "\n",
        "        block = Dropout(dropout)(block)\n",
        "        block = MaxPooling1D(pool_size=pool_size[i])(block)\n",
        "\n",
        "        block = GlobalMaxPool1D()(block)\n",
        "        block = Dense(128, activation='relu')(block)\n",
        "        return block\n",
        "    \n",
        "    def _build_sentence_block(self, max_len_of_sentence, max_num_of_setnence, \n",
        "                              char_dimension=16,\n",
        "                              filters=[[3, 5, 7], [200, 300, 300], [300, 400, 400]], \n",
        "#                               filters=[[100, 200, 200], [200, 300, 300], [300, 400, 400]], \n",
        "                              kernel_sizes=[[4, 3, 3], [5, 3, 3], [6, 3, 3]], \n",
        "                              pool_sizes=[[2, 2, 2], [2, 2, 2], [2, 2, 2]],\n",
        "                              dropout=0.4):\n",
        "        \n",
        "        sent_input = Input(shape=(max_len_of_sentence, ), dtype='int64')\n",
        "        embedded = Embedding(self.num_of_char, char_dimension, input_length=max_len_of_sentence)(sent_input)\n",
        "        \n",
        "        blocks = []\n",
        "        for i, filter_layers in enumerate(filters):\n",
        "            blocks.append(\n",
        "                self._build_character_block(\n",
        "                    block=embedded, filters=filters[i], kernel_size=kernel_sizes[i], pool_size=pool_sizes[i])\n",
        "            )\n",
        "\n",
        "        sent_output = concatenate(blocks, axis=-1)\n",
        "        sent_output = Dropout(dropout)(sent_output)\n",
        "        sent_encoder = Model(inputs=sent_input, outputs=sent_output)\n",
        "\n",
        "        return sent_encoder\n",
        "    \n",
        "    def _build_document_block(self, sent_encoder, max_len_of_sentence, max_num_of_setnence, \n",
        "                             num_of_label, dropout=0.3, \n",
        "                             loss='sparse_categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy']):\n",
        "        doc_input = Input(shape=(max_num_of_setnence, max_len_of_sentence), dtype='int64')\n",
        "        doc_output = TimeDistributed(sent_encoder)(doc_input)\n",
        "\n",
        "        doc_output = Bidirectional(LSTM(128, return_sequences=False, dropout=dropout))(doc_output)\n",
        "\n",
        "        doc_output = Dropout(dropout)(doc_output)\n",
        "        doc_output = Dense(128, activation='relu')(doc_output)\n",
        "        doc_output = Dropout(dropout)(doc_output)\n",
        "        doc_output = Dense(num_of_label, activation='sigmoid')(doc_output)\n",
        "\n",
        "        doc_encoder = Model(inputs=doc_input, outputs=doc_output)\n",
        "        doc_encoder.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
        "        return doc_encoder\n",
        "    \n",
        "    def preporcess(self, labels, char_dict=None, unknown_label='UNK'):\n",
        "        if self.verbose > 3:\n",
        "            print('-----> Stage: preprocess')\n",
        "            \n",
        "        self.build_char_dictionary(char_dict, unknown_label)\n",
        "        self.convert_labels(labels)\n",
        "    \n",
        "    def process(self, df, x_col, y_col, \n",
        "                max_len_of_sentence=None, max_num_of_setnence=None, label2indexes=None, sample_size=None):\n",
        "        if self.verbose > 3:\n",
        "            print('-----> Stage: process')\n",
        "            \n",
        "        if sample_size is None:\n",
        "            sample_size = 1000\n",
        "        if label2indexes is None:\n",
        "            if self.label2indexes is None:\n",
        "                raise Exception('Does not initalize label2indexes. Please invoke preprocess step first')\n",
        "            label2indexes = self.label2indexes\n",
        "        if max_len_of_sentence is None:\n",
        "            max_len_of_sentence = self.max_len_of_sentence\n",
        "        if max_num_of_setnence is None:\n",
        "            max_num_of_setnence = self.max_num_of_setnence\n",
        "\n",
        "        x_preprocess, y_preprocess = self._transform_raw_data(\n",
        "            df=df, x_col=x_col, y_col=y_col, label2indexes=label2indexes)\n",
        "        \n",
        "        x_preprocess, y_preprocess = self._transform_training_data(\n",
        "            x_raw=x_preprocess, y_raw=y_preprocess,\n",
        "            max_len_of_sentence=max_len_of_sentence, max_num_of_setnence=max_num_of_setnence)\n",
        "        \n",
        "        if self.verbose > 5:\n",
        "            print('Shape: ', x_preprocess.shape, y_preprocess.shape)\n",
        "\n",
        "        return x_preprocess, y_preprocess\n",
        "    \n",
        "    def build_model(self, char_dimension=16, display_summary=False, display_architecture=False, \n",
        "                    loss='sparse_categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy']):\n",
        "        if self.verbose > 3:\n",
        "            print('-----> Stage: build model')\n",
        "            \n",
        "        sent_encoder = self._build_sentence_block(\n",
        "            char_dimension=char_dimension,\n",
        "            max_len_of_sentence=self.max_len_of_sentence, max_num_of_setnence=self.max_num_of_setnence)\n",
        "                \n",
        "        doc_encoder = self._build_document_block(\n",
        "            sent_encoder=sent_encoder, num_of_label=self.num_of_label,\n",
        "            max_len_of_sentence=self.max_len_of_sentence, max_num_of_setnence=self.max_num_of_setnence, \n",
        "            loss=loss, optimizer=optimizer, metrics=metrics)\n",
        "        \n",
        "        if display_architecture:\n",
        "            print('Sentence Architecture')\n",
        "            IPython.display.display(SVG(model_to_dot(sent_encoder).create(prog='dot', format='svg')))\n",
        "            print()\n",
        "            print('Document Architecture')\n",
        "            IPython.display.display(SVG(model_to_dot(doc_encoder).create(prog='dot', format='svg')))\n",
        "        \n",
        "        if display_summary:\n",
        "            print(doc_encoder.summary())\n",
        "            \n",
        "        \n",
        "        self.model = {\n",
        "            'sent_encoder': sent_encoder,\n",
        "            'doc_encoder': doc_encoder\n",
        "        }\n",
        "        \n",
        "        return doc_encoder\n",
        "    \n",
        "    def train(self, x_train, y_train, x_test, y_test, batch_size=128, epochs=1, shuffle=True):\n",
        "        if self.verbose > 3:\n",
        "            print('-----> Stage: train model')\n",
        "            \n",
        "        self.get_model().fit(\n",
        "            x_train, y_train, validation_data=(x_test, y_test), \n",
        "            batch_size=batch_size, epochs=epochs, shuffle=shuffle)\n",
        "        \n",
        "#         return self.model['doc_encoder']\n",
        "\n",
        "    def predict(self, x, return_prob=False):\n",
        "        if self.verbose > 3:\n",
        "            print('-----> Stage: predict')\n",
        "            \n",
        "        if return_prob:\n",
        "            return self.get_model().predict(x_test)\n",
        "        \n",
        "        return self.get_model().predict(x_test).argmax(axis=-1)\n",
        "    \n",
        "    def get_model(self):\n",
        "        return self.model['doc_encoder']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVZIc_mcJdZe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "d1348e2e-0aa3-461e-a3cc-7908258b472d"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEKAbnhoIYrZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "710926a8-ee1b-4ffe-f14e-8baeab62f8df"
      },
      "source": [
        "\"\"\"\n",
        "    Maximum number of characters per sentence is 256.\n",
        "    Maximum number of sentence is 5\n",
        "\"\"\"\n",
        "\n",
        "char_cnn = CharCNN(max_len_of_sentence=256, max_num_of_setnence=5)\n",
        "\n",
        "\"\"\"\n",
        "    First of all, we need to prepare meta information including character dictionary \n",
        "    and converting label from text to numeric (as keras support numeric input only).\n",
        "\"\"\"\n",
        "char_cnn.preporcess(labels=df['task_1'].unique())\n",
        "\n",
        "\"\"\"\n",
        "    We have to transform raw input training data and testing to numpy format for keras input\n",
        "\"\"\"\n",
        "x_train, y_train = char_cnn.process(\n",
        "    df=train_df, x_col='text', y_col='task_1')\n",
        "x_test, y_test = char_cnn.process(\n",
        "    df=test_df, x_col='text', y_col='task_1')\n",
        "\n",
        "char_cnn.build_model()\n",
        "char_cnn.train(x_train, y_train, x_test, y_test, batch_size=208, epochs=3)\n",
        "\n",
        "char_cnn.get_model().save('./char_cnn_model.h5')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-----> Stage: preprocess\n",
            "Totoal number of chars: 83\n",
            "First 3 char_indices sample: {'UNK': 0, 'U': 1, 'C': 2}\n",
            "First 3 indices_char sample: {0: 'UNK', 1: 'U', 2: 'C'}\n",
            "Label to Index:  {'task_1': 0, 'NOT': 1, 'HOF': 2}\n",
            "Index to Label:  {0: 'task_1', 1: 'NOT', 2: 'HOF'}\n",
            "-----> Stage: process\n",
            "Number of news: 4682\n",
            "Actual max sentence: 69\n",
            "Shape:  (4682, 5, 256) (4682,)\n",
            "-----> Stage: process\n",
            "Number of news: 1171\n",
            "Actual max sentence: 19\n",
            "Shape:  (1171, 5, 256) (1171,)\n",
            "-----> Stage: build model\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4479: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3622: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "-----> Stage: train model\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 4682 samples, validate on 1171 samples\n",
            "Epoch 1/3\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "4682/4682 [==============================] - 520s 111ms/step - loss: 0.7132 - acc: 0.5942 - val_loss: 0.6764 - val_acc: 0.6208\n",
            "Epoch 2/3\n",
            "4682/4682 [==============================] - 512s 109ms/step - loss: 0.6803 - acc: 0.6106 - val_loss: 0.6693 - val_acc: 0.6208\n",
            "Epoch 3/3\n",
            "4682/4682 [==============================] - 512s 109ms/step - loss: 0.6768 - acc: 0.6113 - val_loss: 0.6765 - val_acc: 0.6208\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}